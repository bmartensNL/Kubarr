name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # =============================================================================
  # Backend Tests (Rust)
  # =============================================================================
  backend-tests:
    name: Backend Tests
    runs-on: self-hosted
    defaults:
      run:
        working-directory: code/backend

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy, rustfmt

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            code/backend/target
          key: ${{ runner.os }}-cargo-${{ hashFiles('code/backend/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Check formatting
        run: cargo fmt --check

      - name: Run clippy
        run: cargo clippy --all-targets --all-features -- -D warnings
        continue-on-error: true  # Don't fail on warnings for now

      - name: Run tests
        run: cargo test --verbose

  # =============================================================================
  # Frontend Lint & Build
  # =============================================================================
  frontend-check:
    name: Frontend Check
    runs-on: self-hosted
    defaults:
      run:
        working-directory: code/frontend

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: code/frontend/package-lock.json

      - name: Install dependencies
        run: npm ci

      - name: Lint
        run: npm run lint

      - name: Build (includes TypeScript check)
        run: npm run build

  # =============================================================================
  # Build Docker Images (once, shared via local Docker daemon)
  # =============================================================================
  build-images:
    name: Build Docker Images
    runs-on: self-hosted

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Build images in parallel
        run: |
          TAG="ci-${{ github.sha }}"

          docker build -f docker/Dockerfile.backend -t kubarr-backend:${TAG} \
            --build-arg COMMIT_HASH=${{ github.sha }} \
            --build-arg BUILD_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ") . &
          PID1=$!

          docker build -f docker/Dockerfile.frontend -t kubarr-frontend:${TAG} \
            --build-arg COMMIT_HASH=${{ github.sha }} \
            --build-arg BUILD_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ") . &
          PID2=$!

          wait $PID1 && echo "Backend image built"
          wait $PID2 && echo "Frontend image built"

  # =============================================================================
  # E2E Tests (Kind cluster + Playwright)
  # =============================================================================
  e2e-tests:
    name: E2E Tests
    needs: build-images
    runs-on: self-hosted
    timeout-minutes: 30
    env:
      KIND_CLUSTER_NAME: kubarr-e2e-${{ github.run_id }}-${{ github.run_attempt }}
      IMAGE_TAG: ci-${{ github.sha }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: code/frontend/package-lock.json

      - name: Install frontend dependencies
        working-directory: code/frontend
        run: npm ci

      - name: Install Playwright browsers
        working-directory: code/frontend
        run: npx playwright install --with-deps chromium

      - name: Create Kind cluster
        run: |
          kind create cluster --name "$KIND_CLUSTER_NAME" --wait 120s
          kubectl cluster-info --context "kind-${KIND_CLUSTER_NAME}"

      - name: Load images into Kind
        run: |
          kind load docker-image "kubarr-backend:${IMAGE_TAG}" --name "$KIND_CLUSTER_NAME"
          kind load docker-image "kubarr-frontend:${IMAGE_TAG}" --name "$KIND_CLUSTER_NAME"

      - name: Generate secrets
        run: |
          openssl genrsa -out /tmp/jwt-private.pem 2048
          openssl rsa -in /tmp/jwt-private.pem -pubout -out /tmp/jwt-public.pem
          echo "COOKIE_SECRET=$(openssl rand -base64 32 | tr -d '\n' | head -c 32)" >> $GITHUB_ENV
          echo "CLIENT_SECRET=e2e-test-client-secret-12345" >> $GITHUB_ENV

      - name: Deploy Kubarr
        run: |
          kubectl create namespace kubarr --dry-run=client -o yaml | kubectl apply -f -

          kubectl create secret generic kubarr-jwt-keys \
            --from-file=private.pem=/tmp/jwt-private.pem \
            --from-file=public.pem=/tmp/jwt-public.pem \
            -n kubarr \
            --dry-run=client -o yaml | kubectl apply -f -

          helm upgrade --install kubarr ./charts/kubarr \
            --namespace kubarr \
            --set namespace.create=false \
            --set backend.image.repository=kubarr-backend \
            --set backend.image.tag=${IMAGE_TAG} \
            --set backend.image.pullPolicy=Never \
            --set frontend.image.repository=kubarr-frontend \
            --set frontend.image.tag=${IMAGE_TAG} \
            --set frontend.image.pullPolicy=Never \
            --set oauth2.enabled=true \
            --set auth.jwt.existingSecret=kubarr-jwt-keys \
            --set networkPolicy.enabled=false \
            --set podSecurityContext.runAsNonRoot=false \
            --set podSecurityContext.runAsUser=0 \
            --set podSecurityContext.fsGroup=0 \
            --wait --timeout=5m

      - name: Wait for pods
        run: |
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=kubarr -n kubarr --timeout=180s
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=kubarr-frontend -n kubarr --timeout=120s
          kubectl get pods -A

      - name: Setup port-forwards
        run: |
          kubectl port-forward svc/kubarr 8000:8000 -n kubarr &
          echo $! > /tmp/pf_backend_pid

          for i in $(seq 1 30); do
            if curl -s http://localhost:8000/api/system/health 2>/dev/null | grep -q "ok"; then
              echo "Backend ready"
              break
            fi
            sleep 2
          done

      - name: Initialize Kubarr setup
        run: |
          curl -s -X POST http://localhost:8000/api/setup/initialize \
            -H "Content-Type: application/json" \
            -d '{
              "admin_username": "admin",
              "admin_email": "admin@test.local",
              "admin_password": "admin",
              "storage_path": "/data",
              "base_url": "http://localhost:8000"
            }'

      - name: Run Playwright tests
        working-directory: code/frontend
        run: npx playwright test --project=chromium
        env:
          BASE_URL: http://localhost:8000
          CI: true

      - name: Upload Playwright report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report
          path: code/frontend/playwright-report/
          retention-days: 14

      - name: Debug logs
        if: failure()
        run: |
          echo "=== Backend Logs ==="
          kubectl logs -l app.kubernetes.io/name=kubarr -n kubarr --tail=50 2>/dev/null || true
          echo "=== Frontend Logs ==="
          kubectl logs -l app.kubernetes.io/name=kubarr-frontend -n kubarr --tail=50 2>/dev/null || true
          echo "=== Pod Descriptions ==="
          kubectl describe pods -n kubarr 2>/dev/null | tail -100 || true

      - name: Cleanup Kind cluster
        if: always()
        run: |
          kill $(cat /tmp/pf_backend_pid) 2>/dev/null || true
          kind delete cluster --name "$KIND_CLUSTER_NAME" || true
